---
title: "Session 9: Scraping Interactive Web Pages"
subtitle: "Introduction to Web Scraping and Data Management for Social Scientists"
author: "Johannes B. Gruber"
date: 2025-07-17
from: markdown+emoji
format:
  revealjs:
    smaller: true
    width: 1600
    height: 900
    scrollable: true
    code-line-numbers: true
    slide-number: c/t
    logo: ../ess_logo.png
    embed-resources: true
bibliography: ../references.bib
execute:
  eval: true
  cache: false
  echo: true
engine: knitr
highlight-style: nord
---

# Introduction
## This Course

<center>
```{r setup}
#| echo: false
#| message: false
library(tinytable)
library(tidyverse)
library(httr2)
tibble::tribble(
  ~Day, ~Session,
  1,  "Introduction",
  2,  "Data Structures and Wrangling",
  3,  "Working with Files",
  4,  "Linking and joining data & SQL",
  5,  "Scaling, Reporting and Database Software",
  6,  "Introduction to the Web",
  7,  "Static Web Pages",
  8,  "Application Programming Interface (APIs) ",
  9,  "Interactive Web Pages",
  10, "Building a Reproducible Research Project",
) |> 
  tt() |> 
  style_tt() |> 
  style_tt(i = 9, background = "#FDE000")
```
</center>

## The Plan for Today

:::: {.columns}

::: {.column width="60%"}
In this session, we learn how to hunt down **wild** data.
We will:

- Learn how to find secret APIs
- Emulate a Browser
- We focus specifically on step 1 below
  
![Original Image Source: prowebscraper.com](../07_Static_Web_Pages/media/web_scraping_steps.png)
:::

::: {.column width="40%" }
![](https://images.unsplash.com/photo-1564166174574-a9666f590437?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=774&q=80)
[Philipp Pilz](https://unsplash.com/@buchstabenhausen) via unsplash.com
:::

::::


# Request & Collect Raw Data: a closer look
## Common Problems

Imagine you wanted to scrape researchgate.net, since it contains self-created profiles of many researchers.
However, when you try to get the html content:

```{r}
#| error: true
library(rvest)
read_html("https://www.researchgate.net/profile/Johannes-Gruber-2")
```

If you don't know what an HTTP error means, you can go to https://http.cat and have the status explained in a fun way.
Below I use a little convenience function:

```{r}
error_cat <- function(error) {
  link <- paste0("https://http.cat/images/", error, ".jpg")
  knitr::include_graphics(link)
}
error_cat(403)
```

## So what's going on?

- If something like this happens, the server essentially did not fullfill our request
- This is because the website seems to have some special requirements for serving the (correct) content. These could be:
  - specific user agents
  - other specific headers
  - login through browser cookies
- To find out how the browser manages to get the correct response, we can use the Network tab in the inspection tool


## Strategy 1: Emulate what the Browser is Doing

Open the Inspect Window Again:

![](media/inspect.png)

## Strategy 1: Emulate what the Browser is Doing {visibility="uncounted"}

But this time, we focus on the *Network* tab:

![](media/copy-curl.png)

Here we get an overview of all the network activity of the browser and the individual requests for data that are performed.
Clear the network log first and reload the page to see what is going on.
Finding the right call is not always easy, but in most cases, we want:

- a call with status 200 (OK/successful)
- a document type 
- something that is at least a few kB in size
- *Initiator* is usually "other" (we initiated the call by refreshing)

When you identified the call, you can right click -> copy -> copy as cURL

## Refresher: `cURL` Calls

:::: {.columns}

::: {.column width="50%"}
What is `cURL`:

- `cURL` is a library that can make HTTP requests.
- it is widely used for API calls from the terminal.
- it lists the parameters of a call in a pretty readable manner:
  - the unnamed argument in the beginning is the Uniform Resource Locator (URL) the request goes to
  - `-H` arguments describe the headers, which are arguments sent with the call
  - `-d` is the data or body of a request, which is used e.g., for uploading things
  - `-o`/`-O` can be used to write the response to a file (otherwise the response is returned to the screen)
  - `--compressed` means to ask for a compressed response which is unpacked locally (saves bandwith)
:::

::: {.column width="50%" }
```{bash}
#| style: "font-size: 110%;"
#| eval: false
curl 'https://www.researchgate.net/profile/Johannes-Gruber-2' \
  -H 'authority: www.researchgate.net' \
  -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' \
  -H 'accept-language: en-GB,en;q=0.9' \
  -H 'cache-control: max-age=0' \
  -H '[Redacted]' \
  -H 'sec-ch-ua: "Chromium";v="115", "Not/A)Brand";v="99"' \
  -H 'sec-ch-ua-mobile: ?0' \
  -H 'sec-ch-ua-platform: "Linux"' \
  -H 'sec-fetch-dest: document' \
  -H 'sec-fetch-mode: navigate' \
  -H 'sec-fetch-site: cross-site' \
  -H 'sec-fetch-user: ?1' \
  -H 'upgrade-insecure-requests: 1' \
  -H 'user-agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36' \
  --compressed
```
:::

::::


## `httr2::curl_translate()` 

- We have seen `httr2::curl_translate()` in action yesterday (and the alternative <https://curlconverter.com/r-httr2/>)
- It can also convert more complicated API calls that make look `R` no diffrent from a regular browser
- (Remember: you need to escape all `"` in the call, press `ctrl` + `F` to open the Find & Replace tool and put `"` in the find `\"` in the replace field and go through all matches except the first and last):

```{r}
library(httr2)
httr2::curl_translate(
"curl 'https://www.researchgate.net/profile/Johannes-Gruber-2' \
  -H 'authority: www.researchgate.net' \
  -H 'accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7' \
  -H 'accept-language: en-GB,en;q=0.9' \
  -H 'cache-control: max-age=0' \
  -H 'cookie: [Redacted]' \
  -H 'sec-ch-ua: \"Chromium\";v=\"115\", \"Not/A)Brand\";v=\"99\"' \
  -H 'sec-ch-ua-mobile: ?0' \
  -H 'sec-ch-ua-platform: \"Linux\"' \
  -H 'sec-fetch-dest: document' \
  -H 'sec-fetch-mode: navigate' \
  -H 'sec-fetch-site: cross-site' \
  -H 'sec-fetch-user: ?1' \
  -H 'upgrade-insecure-requests: 1' \
  -H 'user-agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36' \
  --compressed"
)
```

## 'Emulating' the Browser Request

```{r}
#| error: true
resp <- request("https://www.researchgate.net/profile/Johannes-Gruber-2") |>
  req_headers(
    authority = "www.researchgate.net",
    accept = "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    `accept-language` = "en-GB,en;q=0.9",
    `cache-control` = "max-age=0",
    cookie = "[Redacted]",
    `sec-ch-ua` = "\"Chromium\";v=115\", \"Not/A)Brand\";v=\"99",
    `sec-ch-ua-mobile` = "?0",
    `sec-ch-ua-platform` = "\"Linux\"",
    `sec-fetch-dest` = "document",
    `sec-fetch-mode` = "navigate",
    `sec-fetch-site` = "cross-site",
    `sec-fetch-user` = "?1",
    `upgrade-insecure-requests` = "1",
    `user-agent` = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
  ) |>
  req_perform()
resp
```

![](media/200.png)

## Mission outcome: success

- the result can be converted into an `rvest` object and parsed

```{r}
#| error: true
resp |> 
  resp_body_html() |> 
  html_elements("[data-testid=\"publicProfileStatsCitations\"]") |> 
  html_text()
```


# Example: anyflip
## Goal

:::: {.columns}
::: {.column width="45%"}
- get information stored as "flipping book"
- don't take individual screenshots/pages, but get whole book
:::
::: {.column width="50%" }
[
  ![](media/anyflip.png)
](https://anyflip.com/ogboc/rozd/)
:::
::::

## Scout

- hovering over the page, we find this element
  ```html
  <div class="side-image"   style="position:absolute;z-index:0;width:100%;height:100%;top:0;lef  t:0;">
    <img id="c7ARbTjMdA" alt="c7ARbTjMdA"   src="../files/large/ae105d2ec767250c5e9a4e2b0a48efaf.webp?174011361  2" style="pointer-events: none; width: 100%; height: 100%;   position: absolute; top: 0px; left: 0px; transform-origin: 0% 0%   0px; transform: scale(1);" width="100%" height="100%">
  </div>
  ```
- looks easy enough! `.side-image img` should give us the link!
- hold down `ctrl` and click on the link in the inspect

![](media/anyflip_link.png){.fragment}

## Scout {visibility="uncounted"}

Can we get the actual file?

```{r}
#| classes: fragment
dir.create("data/anyflip", showWarnings = FALSE)
out_file <- paste0("data/anyflip/1_ae105d2ec767250c5e9a4e2b0a48efaf.webp")
curl::curl_download(
  url = "https://online.anyflip.com/ogboc/rozd/files/large/ae105d2ec767250c5e9a4e2b0a48efaf.webp?1740113612", 
  destfile = out_file
)
file.size(out_file)
```

:::{.fragment}
We can!

![](https://media1.tenor.com/m/uQ0cY-ThOxAAAAAC/amazed-jake-peralta.gif){.absolute width="50%"}
:::

## Okay, so where are the rest of the links?

```{r}
read_html("https://anyflip.com/ogboc/rozd/") |> 
  html_elements(".side-image img")
```

:tired_face::tired_face::tired_face:

:::{.fragment}
- checking the URL that `rvest` gets
- then the actual source code in the browser
- result: they have an empty iframe where the image should be
:::

## So where do the image links come from?

:::: {.columns}
::: {.column width="35%"}
How I got here:

1. open network tab
2. select JS filter
3. order by size
4. went from the top down
:::
::: {.column width="60%" }
![](media/anyflip_config.png)
:::
::::


## Get image links

```{r}
book_config <- read_html("https://online.anyflip.com/ogboc/rozd/mobile/javascript/config.js") |>
  as.character() |>
  str_extract("\\{.*\\}") |>
  jsonlite::fromJSON()
lobstr::tree(book_config, max_depth = 1)
```


```{r}
file_names <- book_config |> 
  pluck("fliphtml5_pages", "n") |>
  unlist()
file_names
```

## get images

```{r}
dl_urls <- paste0("https://online.anyflip.com/ogboc/rozd/files/large/", file_names)
out_files <- paste0("data/anyflip/", sprintf("%03d", seq_along(file_names)), "_", file_names)
curl::multi_download(
  dl_urls,
  out_files
)
```

## Finally: get a single PDF

```{r}
# convert each image to a pdf
for (page in out_files) {
  magick::image_read(page) |>
    magick::image_write(path = paste0(page, ".pdf"), format = "pdf")
}

# combine into one pdf
title <- read_html("https://anyflip.com/ogboc/rozd/") |> 
  html_elements("title") |> 
  html_text2() |> 
  janitor::make_clean_names()

pdftools::pdf_combine(
  input = paste0(out_files, ".pdf"), 
  output = paste0("data/anyflip/", title, ".pdf")
)
```

## Mission outcome: success!

:::: {.columns}
::: {.column width="60%"}
To summarise the steps:

1. poking around the website with the inspect tool
2. found an image link that we could download
3. the link was not actually in the HTML source, but must come from somewhere
4. found the request that loads the 'config' JSON, containing all the image names in order
5. guessed the format of the image links from the one link we got + the image names
6. Downloaded the files -> success!
:::
::: {.column width="40%" }
![](media/anyflip_success.png)
:::
::::

## Extra: getting the text data?

- since this is a pdf that was made from combining images, we have **no actual text data**
- we can use Optical Character Recognition (and early version of visual ML models)

```{r}
#| cache: true
pdftools::pdf_ocr_text(
  "anyflip/002_ca317b05c3eeca7309063ce966b4b933.webp.pdf", 
  language = "tha"
)
```


# Example: ICA (International Communication Association) 2023 Conference
## Goal

:::: {.columns}

::: {.column width="45%"}
- Let's say we want to build a database of conference attendance
- So for each conference website we want to get:
  - Speakers
  - (Co-)authors
  - Paper/talk titles
  - Panel (to see who was in the same ones)
:::

::: {.column width="50%" }
[
  ![](media/ica.png)
](https://www.icahdq.org/mpage/ICA23-Program)
:::

::::

## Trying to scrape the programme

- The page looks straightforward enough!
- There is a "Conference Schedule" with links to the individual panels
- The table has a pretty nice class by which we can select it: `class="agenda-content"`

```{r}
#| error: true
#| class: fragment
html <- read_html("https://www.icahdq.org/mpage/ICA23-Program")
```

:::{.fragment}
![](https://media.tenor.com/zlai3JBCvVsAAAAM/mindblown-jonstewart.gif)
:::

## Let's Check our Network Tab

![](media/ica-json.png)

:::{.incremental}
- I noticed a request that takes quite long and retrieves a relatively large object (500kB)
- Clicking on it opens another window showing the response
- Wait, is this a json with the entire conference schedule?
:::

## Translating the `cURL` call

```{r}
curl_translate("curl 'https://whova.com/xems/apis/event_webpage/agenda/public/get_agendas/?event_id=JcQAdK91J0qLUtNxOYUVWFMTUuQgIg3Xj6VIeeyXVR4%3D' \
  -H 'Accept: application/json, text/plain, */*' \
  -H 'Accept-Language: en-GB,en-US;q=0.9,en;q=0.8' \
  -H 'Cache-Control: no-cache' \
  -H 'Connection: keep-alive' \
  -H 'Pragma: no-cache' \
  -H 'Referer: https://whova.com/embedded/event/JcQAdK91J0qLUtNxOYUVWFMTUuQgIg3Xj6VIeeyXVR4%3D/' \
  -H 'Sec-Fetch-Dest: empty' \
  -H 'Sec-Fetch-Mode: cors' \
  -H 'Sec-Fetch-Site: same-origin' \
  -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36' \
  -H 'sec-ch-ua: \"Chromium\";v=\"115\", \"Not/A)Brand\";v=\"99\"' \
  -H 'sec-ch-ua-mobile: ?0' \
  -H 'sec-ch-ua-platform: \"Linux\"' \
  --compressed")
```

## Requesting the json (?)

```{r}
ica_data <- request("https://whova.com/xems/apis/event_webpage/agenda/public/get_agendas/") |>
  req_url_query(
    event_id = "JcQAdK91J0qLUtNxOYUVWFMTUuQgIg3Xj6VIeeyXVR4=",
  ) |>
  req_headers(
    Accept = "application/json, text/plain, */*",
    `Accept-Language` = "en-GB,en-US;q=0.9,en;q=0.8",
    `Cache-Control` = "no-cache",
    `User-Agent` = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
  ) |>
  req_perform() |> 
  resp_body_json()
```

:::{.fragment}
```{r}
object.size(ica_data) |> 
  format("MB")
```


It worked!

![](https://media2.giphy.com/media/Q8IYWnnogTYM5T6Yo0/giphy.gif?cid=ecf05e47a37ontzij6ljf2ztej9d9bytu5qlvgxt8xm74ywq&ep=v1_gifs_search&rid=giphy.gif&ct=g)
:::

## Wrangling with Json

:::: {.columns}

::: {.column width="60%"}
- This json file or the R object it produces is quite intimidating.
- To get to a certain panel on the fourth day, for example, we have to enter this insane path:

```{r}
ica_data[["data"]][["agenda"]][[4]][["time_ranges"]][[3]][[2]][[65]][[1]][["sessions"]][[1]] |> 
  lobstr::tree(max_length = 30)
```

- Essentially, someone pressed a relational database into a list format and we now have to scramble to cope with this monstrosity
:::

::: {.column width="40%" }
![](https://upload.wikimedia.org/wikipedia/en/thumb/f/f7/Jason_Voorhees_%28Ken_Kirzinger%29.jpg/250px-Jason_Voorhees_%28Ken_Kirzinger%29.jpg)
:::

::::


## Parsing the Json

I could not come up with a better method so far.
The only way to extract the data is with a nested `for` loop going through all days and all entries in the object and looking for elements called "sessions".

```{r}
library(tidyverse, warn.conflicts = FALSE)
sessions <- list()

for (day in 1:5) {
  
  times <- ica_data[["data"]][["agenda"]][[day]][["time_ranges"]]
  
  for (l_one in seq_along(pluck(times))) {
    for (l_two in seq_along(pluck(times, l_one))) {
      for (l_three in seq_along(pluck(times, l_one, l_two))) {
        for (l_four in seq_along(pluck(times, l_one, l_two, l_three))) {
          
          session <- pluck(times, l_one, l_two, l_three, l_four, "sessions", 1)
          id <- pluck(session, "id")
          if (!is.null(id)) {
            id <- as.character(id)
            sessions[[id]] <- session
          }
          
        }
      }
    }
  }
}
```

## Parsing the Json data

```{r}
ica_data_df <- tibble(
  panel_id = map_int(sessions, "id"),
  panel_name = map_chr(sessions, "name"),
  time = map_chr(sessions, "calendar_stime"),
  desc = map_chr(sessions, function(s) pluck(s, "desc", .default = NA))
)
ica_data_df
```

## Extracting paper title and authors

Finally we want to parse the HTML in the description column.

```{r}
ica_data_df$desc[100]
```

We can inspect HTML content by writing it to a temporary file and opening it in the browser.
Below is a function that does this automatically for you:

```{r}
#| eval: false
check_in_browser <- function(html) {
  tmp <- tempfile(fileext = ".html")
  writeLines(as.character(html), tmp)
  browseURL(tmp)
}
check_in_browser(ica_data_df$desc[100])
```

![](media/ica_panel.png)

## Extracting paper title and authors using a function

I wrote another function for this.
You can check some of the panels using the browser: `check_in_browser(ica_data_df$desc[100])`.

```{r}
pull_papers <- function(desc) {
  # we extract the html code starting with the papers line
  papers <- str_extract(desc, "<b>Papers: </b>.+$") |> 
    str_remove("<b>Papers: </b><br />") |> 
    # we split the html by double line breaks, since it is not properly formatted as paragraphs
    strsplit("<br /><br />", fixed = TRUE) |> 
    pluck(1)
  
  
  # if there is no html code left, just return NAs
  if (all(is.na(papers))) {
    return(list(list(paper_title = NA, authors = NA)))
  } else {
    # otherwise we loop through each paper
    map(papers, function(t) {
      html <- read_html(t)
      
      # first line is the title
      title <- html |> 
        html_text2() |> 
        str_extract("^.+\n")
      
      # at least authors are formatted italice
      authors <- html_elements(html, "i") |> 
        html_text2()
      
      list(paper_title = title, authors = authors)
    })
  }
}
```

Now we have all the information we wanted:

```{r}
ica_data_df_tidy <- ica_data_df |> 
  slice(-613) |> 
  mutate(papers = map(desc, pull_papers)) |> 
  unnest(papers) |> 
  unnest_wider(papers) |> 
  unnest(authors) |> 
  select(-desc) |> 
  filter(!is.na(authors))
ica_data_df_tidy
```

```{r}
ica_data_df_tidy |> 
  filter(!duplicated(paper_title))
```



## Exercises 1

First, review the material and make sure you have a broad understanding how to:

- look at the requests the browser makes
- understand how you can copy a curl call
- practice how you can translate it into R code
- why we go this route and do not simply use `read_html`

1. Open the ICA site in your browser and inspect the network traffic. Can you identify the call to the programme json?
2. Copy the curl code to R and translate it to get the same 

# Example: X-Twitter
## Goal

:::: {.columns}
::: {.column width="50%"}
1. Tweets from a Twitter profile
2. Get the text, likes, shares and comments
:::
::: {.column width="50%" }
[![](media/twitter.png)](https://x.com/EssexSumSchool){target="_blank"}
:::
::::

## Can we use `rvest`?


```{r}
#| error: true
xhtml <- read_html("https://x.com/elonmusk")
```

At least one of these elements should be here!


## Probing the hidden/internal API

![](media/twitter_API.png)


## Translating a request

:::: {.columns}
::: {.column width="50%"}
```{r}
curl_translate("curl 'https://api.x.com/graphql/Le1DChzkS7ioJH_yEPMi3w/UserTweets?variables=%7B%22userId%22%3A%2244196397%22%2C%22count%22%3A20%2C%22includePromotedContent%22%3Atrue%2C%22withQuickPromoteEligibilityTweetFields%22%3Atrue%2C%22withVoice%22%3Atrue%7D^&features=%7B%22rweb_video_screen_enabled%22%3Afalse%2C%22payments_enabled%22%3Afalse%2C%22profile_label_improvements_pcf_label_in_post_enabled%22%3Atrue%2C%22rweb_tipjar_consumption_enabled%22%3Atrue%2C%22verified_phone_label_enabled%22%3Afalse%2C%22creator_subscriptions_tweet_preview_api_enabled%22%3Atrue%2C%22responsive_web_graphql_timeline_navigation_enabled%22%3Atrue%2C%22responsive_web_graphql_skip_user_profile_image_extensions_enabled%22%3Afalse%2C%22premium_content_api_read_enabled%22%3Afalse%2C%22communities_web_enable_tweet_community_results_fetch%22%3Atrue%2C%22c9s_tweet_anatomy_moderator_badge_enabled%22%3Atrue%2C%22responsive_web_grok_analyze_button_fetch_trends_enabled%22%3Afalse%2C%22responsive_web_grok_analyze_post_followups_enabled%22%3Afalse%2C%22responsive_web_jetfuel_frame%22%3Atrue%2C%22responsive_web_grok_share_attachment_enabled%22%3Atrue%2C%22articles_preview_enabled%22%3Atrue%2C%22responsive_web_edit_tweet_api_enabled%22%3Atrue%2C%22graphql_is_translatable_rweb_tweet_is_translatable_enabled%22%3Atrue%2C%22view_counts_everywhere_api_enabled%22%3Atrue%2C%22longform_notetweets_consumption_enabled%22%3Atrue%2C%22responsive_web_twitter_article_tweet_consumption_enabled%22%3Atrue%2C%22tweet_awards_web_tipping_enabled%22%3Afalse%2C%22responsive_web_grok_show_grok_translated_post%22%3Afalse%2C%22responsive_web_grok_analysis_button_from_backend%22%3Atrue%2C%22creator_subscriptions_quote_tweet_preview_enabled%22%3Afalse%2C%22freedom_of_speech_not_reach_fetch_enabled%22%3Atrue%2C%22standardized_nudges_misinfo%22%3Atrue%2C%22tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled%22%3Atrue%2C%22longform_notetweets_rich_text_read_enabled%22%3Atrue%2C%22longform_notetweets_inline_media_enabled%22%3Atrue%2C%22responsive_web_grok_image_annotation_enabled%22%3Atrue%2C%22responsive_web_grok_community_note_auto_translation_is_enabled%22%3Afalse%2C%22responsive_web_enhance_cards_enabled%22%3Afalse%7D^&fieldToggles=%7B%22withArticlePlainText%22%3Afalse%7D' \
  --compressed \
  -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:140.0) Gecko/20100101 Firefox/140.0' \
  -H 'Accept: */*' \
  -H 'Accept-Language: en-US,en;q=0.5' \
  -H 'Accept-Encoding: gzip, deflate, br, zstd' \
  -H 'content-type: application/json' \
  -H 'authorization: Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA' \
  -H 'x-guest-token: 1945748957512028451' \
  -H 'x-twitter-client-language: en' \
  -H 'x-twitter-active-user: yes' \
  -H 'x-client-transaction-id: /+X/QU0nFKRUwDDhoNpOxN6hcq7l28ulNkNdcfdFEE3U31BgOAEaTqD7IWkFeJUVWBa71vsj/XOKKVUiqywC4VnCqcTD/A' \
  -H 'x-xp-forwarded-for: 07099bf3db1505b55e501219de247298d63e25c7a99657e84445e2b0b408ca3484c1f8b62fe6e09da632121120e31ebee903a69b84d0b97628cb3249588cf1e752bbe7137fa549048abb4e8a2e6be33b425ca540a3563942559e9547c77a87f2d12c7fdf5f2d55f42a342ce6deb594ccf69563ebf09c2f2f874426a71861d43e81a8ca2e17e44ff85031a512c015e925e74e3c82029c9ca206bc1b56c74eca99b481c137a6be132d8847424a594c6598b2860b54ae015ad05050947034615b147623213dc862af21edf26ec464c6bad4a8b6' \
  -H 'Origin: https://x.com' \
  -H 'Sec-GPC: 1' \
  -H 'Connection: keep-alive' \
  -H 'Referer: https://x.com/' \
  -H 'Cookie: guest_id=v1%3A175206553082584585; twtr_pixel_opt_in=Y; __cf_bm=kt7jUopsRK0SahsJuTtuGxY.BWNhKqnaxlCx2CPJF9w-1752737676-1.0.1.1-GUgsMYuLL4uw8QeP5leUchXGdroVhoI8E2HalWnyoDecTcVKHkYfCbzqAUjsodmz48D8flKxs1HsltfUTRYbJPbp9q0spQcCy2.cn56VSXE; gt=1945748957512028451' \
  -H 'Sec-Fetch-Dest: empty' \
  -H 'Sec-Fetch-Mode: cors' \
  -H 'Sec-Fetch-Site: same-site' \
  -H 'TE: trailers'")
```
:::
::: {.column width="50%" }
```{r twitter_req}
#| error: true
twitter_resp <- request("https://api.x.com/graphql/Le1DChzkS7ioJH_yEPMi3w/UserTweets") |>
  req_url_query(
    variables = '{"userId":"44196397","count":20,"includePromotedContent":true,"withQuickPromoteEligibilityTweetFields":true,"withVoice":true}^',
    features = '{"rweb_video_screen_enabled":false,"payments_enabled":false,"profile_label_improvements_pcf_label_in_post_enabled":true,"rweb_tipjar_consumption_enabled":true,"verified_phone_label_enabled":false,"creator_subscriptions_tweet_preview_api_enabled":true,"responsive_web_graphql_timeline_navigation_enabled":true,"responsive_web_graphql_skip_user_profile_image_extensions_enabled":false,"premium_content_api_read_enabled":false,"communities_web_enable_tweet_community_results_fetch":true,"c9s_tweet_anatomy_moderator_badge_enabled":true,"responsive_web_grok_analyze_button_fetch_trends_enabled":false,"responsive_web_grok_analyze_post_followups_enabled":false,"responsive_web_jetfuel_frame":true,"responsive_web_grok_share_attachment_enabled":true,"articles_preview_enabled":true,"responsive_web_edit_tweet_api_enabled":true,"graphql_is_translatable_rweb_tweet_is_translatable_enabled":true,"view_counts_everywhere_api_enabled":true,"longform_notetweets_consumption_enabled":true,"responsive_web_twitter_article_tweet_consumption_enabled":true,"tweet_awards_web_tipping_enabled":false,"responsive_web_grok_show_grok_translated_post":false,"responsive_web_grok_analysis_button_from_backend":true,"creator_subscriptions_quote_tweet_preview_enabled":false,"freedom_of_speech_not_reach_fetch_enabled":true,"standardized_nudges_misinfo":true,"tweet_with_visibility_results_prefer_gql_limited_actions_policy_enabled":true,"longform_notetweets_rich_text_read_enabled":true,"longform_notetweets_inline_media_enabled":true,"responsive_web_grok_image_annotation_enabled":true,"responsive_web_grok_community_note_auto_translation_is_enabled":false,"responsive_web_enhance_cards_enabled":false}^',
    fieldToggles = '{"withArticlePlainText":false}',
  ) |>
  req_cookies_set(
    guest_id = "v1:175206553082584585",
    twtr_pixel_opt_in = "Y",
    `__cf_bm` = "kt7jUopsRK0SahsJuTtuGxY.BWNhKqnaxlCx2CPJF9w-1752737676-1.0.1.1-GUgsMYuLL4uw8QeP5leUchXGdroVhoI8E2HalWnyoDecTcVKHkYfCbzqAUjsodmz48D8flKxs1HsltfUTRYbJPbp9q0spQcCy2.cn56VSXE",
    gt = "1945748957512028451",
  ) |>
  req_headers(
    `User-Agent` = "Mozilla/5.0 (X11; Linux x86_64; rv:140.0) Gecko/20100101 Firefox/140.0",
    Accept = "*/*",
    `Accept-Language` = "en-US,en;q=0.5",
    `Accept-Encoding` = "gzip, deflate, br, zstd",
    `content-type` = "application/json",
    authorization = "Bearer AAAAAAAAAAAAAAAAAAAAANRILgAAAAAAnNwIzUejRCOuH5E6I8xnZz4puTs%3D1Zv7ttfk8LF81IUq16cHjhLTvJu4FA33AGWWjCpTnA",
    `x-guest-token` = "1945748957512028451",
    `x-twitter-client-language` = "en",
    `x-twitter-active-user` = "yes",
    `x-client-transaction-id` = "/+X/QU0nFKRUwDDhoNpOxN6hcq7l28ulNkNdcfdFEE3U31BgOAEaTqD7IWkFeJUVWBa71vsj/XOKKVUiqywC4VnCqcTD/A",
    `x-xp-forwarded-for` = "07099bf3db1505b55e501219de247298d63e25c7a99657e84445e2b0b408ca3484c1f8b62fe6e09da632121120e31ebee903a69b84d0b97628cb3249588cf1e752bbe7137fa549048abb4e8a2e6be33b425ca540a3563942559e9547c77a87f2d12c7fdf5f2d55f42a342ce6deb594ccf69563ebf09c2f2f874426a71861d43e81a8ca2e17e44ff85031a512c015e925e74e3c82029c9ca206bc1b56c74eca99b481c137a6be132d8847424a594c6598b2860b54ae015ad05050947034615b147623213dc862af21edf26ec464c6bad4a8b6",
    Origin = "https://x.com",
    `Sec-GPC` = "1",
    TE = "trailers",
  ) |>
  req_perform()
```
:::
::::

## Parsing the Twitter data

This is the code we developed in session 2. We can use it again to get a clean table with some interesting information

```{r}
#| eval: false
ess_tweets <- twitter_resp |> 
  resp_body_json()

entries <- pluck(ess_tweets, "data", "user", "result", "timeline", "timeline", "instructions", 3L, "entries")

tweets <- map(entries, function(x) pluck(x, "content", "itemContent", "tweet_results", "result", "legacy"))

tweets_df <- map(tweets, function(t) {
  tibble(
    id = t$id_str,
    user_id = t$user_id_str,
    created_at = lubridate::parse_date_time(t$created_at, "a b d H M S z Y"),
    full_text = t$full_text,
    favorite_count = t$favorite_count,
    retweet_count = t$retweet_count,
    bookmark_count = t$bookmark_count
  )
}) |> 
  bind_rows()
tweets_df
```



## Mission failure

I stopped at this point since there are three issue that are unclear to resolve:

1. How do we get more than 98 tweets (i.e., scroll the "cursor")?
2. How do we find the user ID?
3. We have to send several identifiers: It is not clear how stable x-csrf-token, authorization, and the cookies are

# Summary: hidden APIs
## What are they

- used by services of a company to communicate with each other
- code on a website often uses one to download additional conent
- the browser logs them and provides them to us as cURL calls

## What are they good for?


:::: {.columns}

::: {.column width="60%"}
- We can often use them to get content that is otherwise unavailable
- We can study them to find out what requests the website server accepts
- Some websites allow access just using a special header or cookies
- If they are somewhat flexible we can wrap them in a function or package
- This can allow us to gather data on scale
:::

::: {.column width="40%" }
![](https://raw.githubusercontent.com/JBGruber/traktok/main/man/figures/logo.png)
:::

::::

## Issues

:::: {.columns}

::: {.column width="40%" .incremental}
- Companies have mechanisms to counter scraping:
  - signing specific requests (TikTok)
  - obscuring pagination (Twitter)
  - rate limiting requests per second/minute/day and user/IP(Twitter)
  - expiring session tokens (telegraaf.nl)
:::

::: {.column width="60%"}
![](https://hackernoon.com/hn-images/0*MPt2rectMhwklT63.jpg)
:::
::::

# Wrap Up

Save some information about the session for reproducibility.

```{r}
sessionInfo()
```
